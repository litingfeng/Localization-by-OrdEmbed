nohup: ignoring input
wandb: Currently logged in as: amberberli (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.9
wandb: Syncing run ordinal_ae_fewshot_digit2_supp500_ent05_trainpatch_shufprotoemb_shufprotoRew_ftclutter
wandb: ⭐️ View project at https://wandb.ai/amberberli/selfpaced
wandb: 🚀 View run at https://wandb.ai/amberberli/selfpaced/runs/til3r7cu
wandb: Run data is saved locally in wandb/run-20210603_093732-til3r7cu
wandb: Run `wandb off` to turn off syncing.
loading moreclutter data... 
loading moreclutter data... 
total train image:  5958  test image:  1032
load  encoder.0.weight
load  encoder.0.bias
load  encoder.2.weight
load  encoder.2.bias
load  encoder.4.weight
load  encoder.4.bias
load  RCNN_top.0.weight
load  RCNN_top.0.bias
load  RCNN_top.3.weight
load  RCNN_top.3.bias
loaded from pretrain_ae_ord_proj_50_lr1e03_scp5_digit3_lamb01_randpatch_stp80_shuffle_proto/last.pth.tar
ckpt epoch 149 acc 92.77%
loading ckpt from  /research/cbim/vast/tl601/results/selfpaced/ordinal/ordinal_ae_proj_seq10_ent6_lr1e03_stp80_50_digit3_randpatch_shufprotoemb_shufprotoRew_supp5/best.pth.tar
=> loaded checkpoint epoch 227 98.0197982788086
/research/cbim/vast/tl601/anaconda3/envs/torch_ml/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown
  len(cache))
